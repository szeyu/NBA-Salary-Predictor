{"cells":[{"cell_type":"markdown","metadata":{"id":"Mayej2yAweYT"},"source":["# Data Mining\n","\n","Data mining is the practice of examining data sources in order to generate new information. First, we will get a feel of web crawling/scraping by extracting some living information on the web. Last but not the least, we are going to see how data APIs generally work."]},{"cell_type":"markdown","metadata":{"id":"4NYXSlkUweYY"},"source":["## 1. Web Scraping with BeautifulSoup\n","\n","BeautifulSoup is a python library that comes with a lot of handy functions for web scraping and gathering information from the internet. There are so many things you can do with BeutifulSoup, but in this notebook, I'll show you a rather specific example of how BeutifulSoup can be applied for data mining.\n","\n","To this, we will use the Eastern Iowa - Cedar Rapids Airport website as an example. There, they provide a real-time flight status update for travellers (https://flycid.com/flight-status/). Let's click and open this website and see how it look like.\n","\n","![Cedar Rapids Airport Webpage](https://github.com/stephenbaek/bigdata/blob/master/in-class-assignments/ica03/figures/cid_web.png?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"lSxH0WjHweYY"},"source":["### 1.1. Anatomy of a Web Page\n","\n","Different people would have different approaches, but what I usually do is to take a look at the anatomy of the web page using my web browser's developer tool. If you use Chrome or Firefox, the developer tool can be opened by pressing `ctrl (cmd) + shift + I` or `F12`. If you use Safari, it is called Web Inspector, and can be opened with `cmd + shift + I`. For other web browsers, there should be a menu somewhere, or an instruction on the internet.\n","\n","Now, in the developer tool, you should find some scripts which define the web page. In Chrome, it looks like this:\n","\n","![Developer Tools in Chrome](https://github.com/stephenbaek/bigdata/blob/master/in-class-assignments/ica03/figures/dev_tools.png?raw=1)\n","\n","The script here looks a lot like XML. It is in fact called Hypertext Markup Language, or HTML, which is a standard markup language for web documents. You don't have to know all the tags of HTML. However, if you are curious about some basic HTML tags, here's a [nice summary of most commonly used HTML tags](https://www.geeksforgeeks.org/most-commonly-used-tags-in-html/).\n","\n","Now, most web browsers highlights a specific part of web document when you hover a mouse cursor over a script in the developer tool, like in the screenshot below.\n","\n","\n","This is where your job as a data scientist gets less elegant but a little dirty and brute force (welcome to the real world!): The first thing to do to extract an information from a web document is to figure out exactly where the desired information is located. In this example, after a few minutes of digging in (basically hovering the mouse cursor on different locations of the HTML scripts), I found that the flight information was being displayed as an `iframe`, which is basically like a web page within a web page.\n","\n","\n","What this means is that the airport website is not actually doing anything by itself to retrieve the flight information, but instead, displays an external web page (https://webservice.prodigiq.com/wfids/CID/small?rows=18) within the airport web page as if it is a part of the web page. Long story short, this is where all the desired information we need and, hence, where we will do the web scraping."]},{"cell_type":"markdown","metadata":{"id":"hy8EqtSEweYY"},"source":["### 2.2. Get and Parse HTML\n","\n","Now that we know where the information exists, let's retrieve the HTML tags and parse them into a useful information for us. First off, let's retrieve the entire web page."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e3rMLm_nweYY"},"outputs":[],"source":["import requests\n","page = requests.get(\"https://webservice.prodigiq.com/wfids/CID/small?rows=18\")"]},{"cell_type":"markdown","metadata":{"id":"UroI8CoDweYY"},"source":["Now, with BeutifulSoup, we parse the information and display it in the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yK-3Qm4OweYZ"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","soup = BeautifulSoup(page.content, 'html.parser')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33rEBwpKweYZ"},"outputs":[],"source":["print(soup.prettify())"]},{"cell_type":"markdown","metadata":{"id":"UgbQvblNweYZ"},"source":["There are a lot of things going on, but after another dirty work of digging into the tags, we can find the flight information table lives in the tag `table` with an attribute `class=\"views-table cols-5\"`, which can be searched by BeutifulSoup:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcQaJwCDweYZ"},"outputs":[],"source":["table = soup.find('table', {'class': 'views-table cols-5'})\n","print(table)"]},{"cell_type":"markdown","metadata":{"id":"c5yh31TJweYZ"},"source":["Furthermore, within the table, it seems like all the flight information is structured within `tbody` tag."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1slL0OxhweYZ"},"outputs":[],"source":["tbody = table.find('tbody')\n","tbody"]},{"cell_type":"markdown","metadata":{"id":"fyY_4h97weYZ"},"source":["For further break down, each flight now is within `tr` task. So, we are going to find all `tr` tags in `tbody` and create a list. Just as a crash HTML course, `tr` is an abbreviation for 'table row' while `td` is for 'table data (column)'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qy-53tSUweYZ"},"outputs":[],"source":["trows = tbody.find_all('tr')\n","trows[0]"]},{"cell_type":"markdown","metadata":{"id":"hqn7y3YGweYZ"},"source":["As we can see, each table row (`tr`) contains multiple table data (`td`). In this case, the first (counting from zero) `td` tag contains the flight number, the second contains the city of departure, the third the arrival time, the fourth baggage claim, and last the arrival status:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0QLwohjweYZ","scrolled":true},"outputs":[],"source":["print('{:10s} | {:15s} | {:15s} | {:15s} | {:10s}'.format(\n","    'Flight', 'Departure City', 'Arrival Time', 'Baggage Claim', 'Status'))\n","for i, trow in enumerate(trows):\n","    titems = trow.find_all('td')   # find all the data items in each row\n","    print('{:10s} | {:15s} | {:15s} | {:15s} | {:10s}'.format(\n","        titems[1].contents[0],          # contents of the first table item (column)\n","        titems[2].contents[0],          # contents of the second table item\n","        titems[3].contents[0],          # contents of the third table item\n","        titems[4].contents[0],          # contents of the fourth table item\n","        titems[5].contents[0],          # contents of the fifth table item\n","    ))"]},{"cell_type":"markdown","metadata":{"id":"u5zRfgYYweYa"},"source":["There, now you can retrieve the real-time flight arrival information at the Cedar Rapids Airport!"]},{"cell_type":"markdown","metadata":{"id":"WpkGoNMpweYa"},"source":["## 2. Get Live Stock Price using `yahoo_fin` API\n","\n","As we have seen above, writing a scraping/crawling code from scratch involves a lot of dirty, brute-force works. For many cases, however, there are people who have already gone through all these and quite generously decided to build a set of handy functions that let you skip all those hassles. Or, sometimes, engineers and developers at companies, who actually built the web pages and knows exactly how the information is structured, decided to provide \"nerd users\" like me ways to access their data. Whatever the reason was, a data set API is basically a predefind set of functions that helps you access the data.\n","\n","In this quick example, we are going to retrive real-time stock price data using Yahoo! Finance API (`yahoo_fin`). Let us first install `yahoo_fin` API."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1eifDmFQweYa"},"outputs":[],"source":["!pip install --upgrade yahoo_fin"]},{"cell_type":"markdown","metadata":{"id":"0tvsAaPZweYa"},"source":["Now, `yahoo_fin` library comes with lots of modules in it. Among them, in this example, we are going to use `stock_info` module. Importing it should look like this:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5vYqXyaweYd"},"outputs":[],"source":["from yahoo_fin import stock_info as si"]},{"cell_type":"markdown","metadata":{"id":"_rO5t3FXweYd"},"source":["For data set APIs, we are not going to get too much into details, but here are several things you can do to retrieve real-time stock info."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Fp-7JxiweYd"},"outputs":[],"source":["# Get Netflix (NFLX) stock info from year 2015 to 2018\n","data = si.get_data('NFLX', start_date='01/01/2015', end_date='12/31/2018')\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DUIXV48IweYd"},"outputs":[],"source":["data.index # gives time stamps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VtYsCUyvweYd"},"outputs":[],"source":["data['volume'].values  # gives values of the column named 'volume'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvL__SG4weYd"},"outputs":[],"source":["data[['open','close']].values # gives multiple columns"]}],"metadata":{"@webio":{"lastCommId":null,"lastKernelId":null},"colab":{"provenance":[{"file_id":"1adH6Ca-QNIbntjiD3-Bwle1J4FAGl5AF","timestamp":1711329685223}]},"kernelspec":{"display_name":"Python [conda env:bigdata]","language":"python","name":"conda-env-bigdata-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}